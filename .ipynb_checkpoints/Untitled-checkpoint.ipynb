{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "384b3d60-a867-479c-bd78-27b8dec91457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"graph_outputs/one2seq_semeval_all_output.json\", 'r') as f:\n",
    "    dic = json.load(f)\n",
    "\n",
    "scores = dic['scores']\n",
    "predictions = dic['predictions']\n",
    "entropies = dic['entropies']\n",
    "context_lines = dic['context_lines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f82c3099-cf83-45e5-ada8-6ea2a2ba4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "066f49d9-4830-45f5-b8eb-de81de744e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scalable grid service discovery based on uddi . efficient discovery of grid services is essential for the success of grid computing . the standardization of grids based on web services has resulted in the need for scalable web service discovery mechanisms to be deployed in grids even though uddi has been the de facto industry standard for web services discovery , imposed requirements of tight replication among registries and lack of autonomous control has severely hindered its widespread deployment and usage . with the advent of grid computing the scalability issue of uddi will become a roadblock that will prevent its deployment in grids . in this paper we present our distributed web service discovery architecture , called dude ( distributed uddi deployment engine ) . dude leverages dht ( distributed hash tables ) as a rendezvous mechanism between multiple uddi registries . dude enables consumers to query multiple registries , still at the same time allowing organizations to have autonomous control over their registries . . based on preliminary prototype on planetlab , we believe that dude architecture can support effective distribution of uddi registries thereby making uddi more robust and also addressing its scaling issues . furthermore , the dude architecture for scalable distribution can be applied beyond uddi to any grid service discovery mechanism . categories and subject descriptors c2 . <digit> distributed systems general terms design , experimentation , standardization .',\n",
       " 'self adaptive applications on the grid . grids are inherently heterogeneous and dynamic . one important problem in grid computing is resource selection , that is , finding an appropriate resource set for the application . another problem is adaptation to the changing characteristics of the grid environment . existing solutions to these two problems require that a performance model for an application is known . however , constructing such models is a complex task . in this paper , we investigate an approach that does not require performance models . we start an application on any set of resources . during the application run , we periodically collect the statistics about the application run and deduce application requirements from these statistics . then , we adjust the resource set to better fit the application needs . this approach allows us to avoid performance bottlenecks , such as overloaded wan links or very slow processors , and therefore can yield significant performance improvements . we evaluate our approach in a number of scenarios typical for the grid . categories and subject descriptors c . <digit> . <digit> computer communication networks distributed systems distributed applications c . <digit> performance of systems measurement techniques , modelling techniques general terms algorithms , measurement , performance , experimentation']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_lines[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "926b1ebf-e810-4576-b2cd-aad97856d000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['grid',\n",
       "  'service',\n",
       "  'discovery',\n",
       "  '<sep>',\n",
       "  'uddi',\n",
       "  '<sep>',\n",
       "  'grid',\n",
       "  'computing',\n",
       "  '<sep>',\n",
       "  'web',\n",
       "  'service',\n",
       "  '<sep>',\n",
       "  'distributed',\n",
       "  'hash',\n",
       "  'tables',\n",
       "  '<sep>',\n",
       "  'distributed',\n",
       "  'hash',\n",
       "  'tables',\n",
       "  '<sep>',\n",
       "  'grid',\n",
       "  'computing'],\n",
       " ['grid',\n",
       "  'computing',\n",
       "  '<sep>',\n",
       "  'performance',\n",
       "  'modeling',\n",
       "  '<sep>',\n",
       "  'performance',\n",
       "  'evaluation',\n",
       "  '<sep>',\n",
       "  'adaptive',\n",
       "  'systems']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94f976ca-7e28-4bcd-b01e-8ce10dd256db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama(dataset):\n",
    "    with open('graph_outputs/'+dataset+'_llama3_kpp_test.json') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    kp_predictions = json_data['dup_removed_kp_predictions']\n",
    "    probabilities = json_data['probabilities']\n",
    "    token_predictions = json_data['dup_removed_token_predictions']\n",
    "    src = json_data['src']\n",
    "    targets = json_data['targets']\n",
    "    all_kpp_values = json_data['dup_removed_kpp']\n",
    "\n",
    "    return kp_predictions, probabilities, token_predictions, src, targets, all_kpp_values\n",
    "\n",
    "\n",
    "kp_predictions, probabilities, token_predictions, src, targets, all_kpp_values=load_llama(\"semeval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5339ab2-3a07-4dbe-a24a-a8c1119a32a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Web', ' search', ' interaction'],\n",
       "  [' popular', ' destinations'],\n",
       "  [' search', ' results'],\n",
       "  [' user', ' behavior'],\n",
       "  [' query', '-level', ' granularity'],\n",
       "  [' web', ' search'],\n",
       "  [' search', ' interaction', ' feature'],\n",
       "  [' authority', ' resources'],\n",
       "  [' information', ' needs'],\n",
       "  [' explor', 'atory', ' tasks'],\n",
       "  [' query', ' topic']],\n",
       " [['Trading', ' Networks'],\n",
       "  [' Price', '-', 'Setting', ' Agents'],\n",
       "  [' Nash', ' Equ', 'ilibrium'],\n",
       "  [' Game', ' Theory'],\n",
       "  [' Market', ' Efficiency'],\n",
       "  [' Strategic', ' Pricing'],\n",
       "  [' Graph', ' Theory'],\n",
       "  [' Matching', ' Market'],\n",
       "  [' Social', ' Opt', 'imal', ' Allocation'],\n",
       "  [' Graph', ' Structure'],\n",
       "  [' Competition', ' among', ' Tr', 'aders']]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_predictions[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "800088ce-bbb6-4df7-b7a0-7ef5c9f750dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sep_and_kpp(token_predictions, all_kpp_values):\n",
    "    new_token_list = []\n",
    "    new_kpp_list = []\n",
    "\n",
    "    for tokens, kpp_values in zip(token_predictions, all_kpp_values):\n",
    "        merged_tokens = []\n",
    "        merged_kpp = []\n",
    "        kpp_index = 0\n",
    "\n",
    "        for i, sublist in enumerate(tokens):\n",
    "            merged_tokens.extend(sublist)\n",
    "            merged_kpp.extend(kpp_values[kpp_index:kpp_index + len(sublist)])\n",
    "            kpp_index += len(sublist)\n",
    "\n",
    "            if i < len(tokens) - 1:\n",
    "                merged_tokens.append('<sep>')\n",
    "                merged_kpp.append(0)  # Add corresponding 0 for <sep>\n",
    "\n",
    "        new_token_list.append(merged_tokens)\n",
    "        new_kpp_list.append(merged_kpp)\n",
    "\n",
    "    return new_token_list, new_kpp_list\n",
    "new_token_predictions, new_kpp_values = add_sep_and_kpp(token_predictions, probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e735476-275c-4c09-92a7-e40aae32dd9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Web',\n",
       " ' search',\n",
       " ' interaction',\n",
       " '<sep>',\n",
       " ' popular',\n",
       " ' destinations',\n",
       " '<sep>',\n",
       " ' search',\n",
       " ' results',\n",
       " '<sep>',\n",
       " ' user',\n",
       " ' behavior',\n",
       " '<sep>',\n",
       " ' query',\n",
       " '-level',\n",
       " ' granularity',\n",
       " '<sep>',\n",
       " ' web',\n",
       " ' search',\n",
       " '<sep>',\n",
       " ' search',\n",
       " ' interaction',\n",
       " ' feature',\n",
       " '<sep>',\n",
       " ' authority',\n",
       " ' resources',\n",
       " '<sep>',\n",
       " ' information',\n",
       " ' needs',\n",
       " '<sep>',\n",
       " ' explor',\n",
       " 'atory',\n",
       " ' tasks',\n",
       " '<sep>',\n",
       " ' query',\n",
       " ' topic']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_token_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "484833e7-c1df-47d8-a3b6-90bf3563f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predictions(predictions, beam=False):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    processed_predictions = []\n",
    "    for beam_prediction in predictions:\n",
    "        if beam:\n",
    "            prediction_ = \"\"\n",
    "            for prediction in beam_prediction:\n",
    "                prediction = prediction.replace(\";\", \"<sep>\")\n",
    "                prediction = prediction.split(\"<eos>\")[0]\n",
    "                if not prediction_:\n",
    "                    prediction_ += prediction\n",
    "                else:\n",
    "                    prediction_ += ' <sep> ' + prediction\n",
    "            prediction = prediction_\n",
    "        else:\n",
    "            beam_prediction = beam_prediction.replace(\";\", \"<sep>\")\n",
    "            prediction = beam_prediction.split(\"<eos>\")[0]\n",
    "\n",
    "        prediction = prediction.split(\",\")\n",
    "\n",
    "        stemed_prediction = []\n",
    "        for kp in prediction:\n",
    "            kp = kp.lower().strip()\n",
    "            if kp != \"\" and kp != \"<peos>\" and kp!=\",\" and kp != \".\" and kp != \"<unk>\":  # and \".\" not in kp and \",\" not in kp\n",
    "                tokenized_kp = kp.split(\" \")  # nltk.word_tokenize(kp)\n",
    "                tokenized_stemed_kp = [stemmer.stem(kw).strip() for kw in tokenized_kp]\n",
    "                stemed_kp = \" \".join(tokenized_stemed_kp).replace(\"< digit >\", \"<digit>\")\n",
    "                if stemed_kp.strip() != \"\":\n",
    "                    stemed_prediction.append(stemed_kp.strip())\n",
    "\n",
    "        # make prediction duplicates free but preserve order for @topk\n",
    "\n",
    "        prediction_dict = {}\n",
    "        stemed_prediction_ = []\n",
    "        for kp in stemed_prediction:\n",
    "            if kp not in prediction_dict:\n",
    "                prediction_dict[kp] = 1\n",
    "                stemed_prediction_.append(kp)\n",
    "        stemed_prediction = stemed_prediction_\n",
    "\n",
    "        processed_predictions.extend(stemed_prediction)\n",
    "\n",
    "    return processed_predictions\n",
    "\n",
    "def process_srcs(srcs):\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_srcs = []\n",
    "    tokenized_src = srcs.split()  # Split the string into words\n",
    "    tokenized_stemed_src = [stemmer.stem(token.strip().lower()).strip() for token in tokenized_src]\n",
    "    stemed_src = \" \".join(tokenized_stemed_src).strip()\n",
    "    processed_srcs.append(stemed_src)\n",
    "    return processed_srcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3f93f6b0-a0fc-48d6-bda1-d7bead016115",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=['Web',\n",
    " ' search',\n",
    " ' interaction',\n",
    " '<sep>',\n",
    " ' popular',\n",
    " ' destinations',\n",
    " '<sep>',\n",
    " ' search',\n",
    " ' results',\n",
    " '<sep>',\n",
    " ' user',\n",
    " ' behavior',\n",
    " '<sep>',\n",
    " ' query',\n",
    " '-level',\n",
    " ' granularity',\n",
    " '<sep>',\n",
    " ' web',\n",
    " ' search',\n",
    " '<sep>',\n",
    " ' search',\n",
    " ' interaction',\n",
    " ' feature',\n",
    " '<sep>',\n",
    " ' authority',\n",
    " ' resources',\n",
    " '<sep>',\n",
    " ' information',\n",
    " ' needs',\n",
    " '<sep>',\n",
    " ' explor',\n",
    " 'atory',\n",
    " ' tasks',\n",
    " '<sep>',\n",
    " ' query',\n",
    " ' topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d54659c8-6aa2-451e-8139-05c0a01e472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predictions(predictions, beam=False):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    processed_predictions = []\n",
    "    for beam_prediction in predictions:\n",
    "        if beam:\n",
    "            prediction_ = \"\"\n",
    "            for prediction in beam_prediction:\n",
    "                prediction = prediction.replace(\";\", \"<sep>\")\n",
    "                prediction = prediction.split(\"<eos>\")[0]\n",
    "                if not prediction_:\n",
    "                    prediction_ += prediction\n",
    "                else:\n",
    "                    prediction_ += ' <sep> ' + prediction\n",
    "            prediction = prediction_\n",
    "        else:\n",
    "            beam_prediction = beam_prediction.replace(\";\", \"<sep>\")\n",
    "            prediction = beam_prediction.split(\"<eos>\")[0]\n",
    "\n",
    "        prediction = prediction.split(\",\")\n",
    "\n",
    "        stemed_prediction = []\n",
    "        for kp in prediction:\n",
    "            kp = kp.lower().strip()\n",
    "            if kp != \"\" and kp != \"<peos>\" and kp!=\",\" and kp != \".\" and kp != \"<unk>\":  # and \".\" not in kp and \",\" not in kp\n",
    "                tokenized_kp = kp.split(\" \")  # nltk.word_tokenize(kp)\n",
    "                tokenized_stemed_kp = [stemmer.stem(kw).strip() for kw in tokenized_kp]\n",
    "                stemed_kp = \" \".join(tokenized_stemed_kp).replace(\"< digit >\", \"<digit>\")\n",
    "                if stemed_kp.strip() != \"\":\n",
    "                    stemed_prediction.append(stemed_kp.strip())\n",
    "\n",
    "        # make prediction duplicates free but preserve order for @topk\n",
    "\n",
    "        prediction_dict = {}\n",
    "        stemed_prediction_ = []\n",
    "        for kp in stemed_prediction:\n",
    "            if kp not in prediction_dict:\n",
    "                prediction_dict[kp] = 1\n",
    "                stemed_prediction_.append(kp)\n",
    "        stemed_prediction = stemed_prediction_\n",
    "\n",
    "        processed_predictions.extend(stemed_prediction)\n",
    "\n",
    "    return processed_predictions\n",
    "\n",
    "def process_srcs(srcs):\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_srcs = []\n",
    "    tokenized_src = srcs.split()  # Split the string into words\n",
    "    tokenized_stemed_src = [stemmer.stem(token.strip().lower()).strip() for token in tokenized_src]\n",
    "    stemed_src = \" \".join(tokenized_stemed_src).strip()\n",
    "    processed_srcs.append(stemed_src)\n",
    "    return processed_srcs\n",
    "\n",
    "def probab_llama_boxplots(dataset): #taken from line 811 one2seq code and adapted for llama\n",
    "\n",
    "    kp_predictions, probabilities, token_predictions, src, targets, all_kpp_values=load_llama(dataset)\n",
    "    context_lines=src\n",
    "    token_predictions,scores=add_sep_and_kpp(token_predictions, probabilities)\n",
    "    \n",
    "    predictions=[]\n",
    "    srcs=[]\n",
    "    \n",
    "    for i in range(len(src)):\n",
    "        temp_predictions = process_predictions(token_predictions[i])\n",
    "        #temp_srcs = process_srcs(src[i])\n",
    "        predictions.append(temp_predictions)\n",
    "        #srcs.append(temp_srcs)\n",
    "    print(predictions[0:2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5645ca97-57c7-4a03-aef6-69a8cd589732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['web', 'search', 'interact', '<sep>', 'popular', 'destin', '<sep>', 'search', 'result', '<sep>', 'user', 'behavior', '<sep>', 'queri', '-level', 'granular', '<sep>', 'web', 'search', '<sep>', 'search', 'interact', 'featur', '<sep>', 'author', 'resourc', '<sep>', 'inform', 'need', '<sep>', 'explor', 'atori', 'task', '<sep>', 'queri', 'topic'], ['trade', 'network', '<sep>', 'price', '-', 'set', 'agent', '<sep>', 'nash', 'equ', 'ilibrium', '<sep>', 'game', 'theori', '<sep>', 'market', 'effici', '<sep>', 'strateg', 'price', '<sep>', 'graph', 'theori', '<sep>', 'match', 'market', '<sep>', 'social', 'opt', 'imal', 'alloc', '<sep>', 'graph', 'structur', '<sep>', 'competit', 'among', 'tr', 'ader']]\n"
     ]
    }
   ],
   "source": [
    "probab_llama_boxplots(\"semeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa798a5e-73bb-4dd8-97e1-f4a9e67878ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "[['grid',\n",
    "  'service',\n",
    "  'discovery',\n",
    "  '<sep>',\n",
    "  'uddi',\n",
    "  '<sep>',\n",
    "  'grid',\n",
    "  'computing',\n",
    "  '<sep>',\n",
    "  'web',\n",
    "  'service',\n",
    "  '<sep>',\n",
    "  'distributed',\n",
    "  'hash',\n",
    "  'tables',\n",
    "  '<sep>',\n",
    "  'distributed',\n",
    "  'hash',\n",
    "  'tables',\n",
    "  '<sep>',\n",
    "  'grid',\n",
    "  'computing'],\n",
    " ['grid',\n",
    "  'computing',\n",
    "  '<sep>',\n",
    "  'performance',\n",
    "  'modeling',\n",
    "  '<sep>',\n",
    "  'performance',\n",
    "  'evaluation',\n",
    "  '<sep>',\n",
    "  'adaptive',\n",
    "  'systems']]\n",
    "token_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66bb7c66-b4c5-4c6b-8427-3c611ee15515",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "srcs=[]\n",
    "\n",
    "for i in range(len(src)):\n",
    "    temp_predictions = process_predictions(token_predictions[i][0])\n",
    "    temp_srcs = process_srcs(src[i])\n",
    "    predictions.append(temp_predictions)\n",
    "    srcs.append(temp_srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e20b87b-64ab-421a-b00c-41f2122d8c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Web', ' search', ' interaction']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee3ff8-c514-46e2-8160-c8e1d9f9e9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91f080e8-4ba8-4970-ba40-f511abc594f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_list=combine_with_separator(token_predictions)\n",
    "sep_kpp=combine_with_separator1(all_kpp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06b1ea8e-f7f0-4ecc-9afa-89826cbd1c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.1497876701364411,\n",
       "  1.1143242899866512,\n",
       "  2.3891409365149947,\n",
       "  1.7717787304326484,\n",
       "  1.2276765139403374,\n",
       "  5.770539652041531,\n",
       "  1.9125475609114326,\n",
       "  3.736857856164639,\n",
       "  3.7247930349605762,\n",
       "  1.0913318776857792,\n",
       "  3.0313200339968684,\n",
       "  '<sep>',\n",
       "  0],\n",
       " [1.5681105111721405,\n",
       "  1.0073730011965898,\n",
       "  4.972799635990106,\n",
       "  2.3086282395514695,\n",
       "  2.849925088934097,\n",
       "  6.806432206131389,\n",
       "  1.832622051593755,\n",
       "  1.7361748951641185,\n",
       "  3.780131091318533,\n",
       "  9.331080740925575,\n",
       "  1.1584410684403077,\n",
       "  '<sep>',\n",
       "  0]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep_kpp[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5171a0ac-129e-496a-99aa-5e6a35a25a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Web',\n",
       "  ' search',\n",
       "  ' interaction',\n",
       "  '<sep>',\n",
       "  ' popular',\n",
       "  ' destinations',\n",
       "  '<sep>',\n",
       "  ' search',\n",
       "  ' results',\n",
       "  '<sep>',\n",
       "  ' user',\n",
       "  ' behavior',\n",
       "  '<sep>',\n",
       "  ' query',\n",
       "  '-level',\n",
       "  ' granularity',\n",
       "  '<sep>',\n",
       "  ' web',\n",
       "  ' search',\n",
       "  '<sep>',\n",
       "  ' search',\n",
       "  ' interaction',\n",
       "  ' feature',\n",
       "  '<sep>',\n",
       "  ' authority',\n",
       "  ' resources',\n",
       "  '<sep>',\n",
       "  ' information',\n",
       "  ' needs',\n",
       "  '<sep>',\n",
       "  ' explor',\n",
       "  'atory',\n",
       "  ' tasks',\n",
       "  '<sep>',\n",
       "  ' query',\n",
       "  ' topic'],\n",
       " ['Trading',\n",
       "  ' Networks',\n",
       "  '<sep>',\n",
       "  ' Price',\n",
       "  '-',\n",
       "  'Setting',\n",
       "  ' Agents',\n",
       "  '<sep>',\n",
       "  ' Nash',\n",
       "  ' Equ',\n",
       "  'ilibrium',\n",
       "  '<sep>',\n",
       "  ' Game',\n",
       "  ' Theory',\n",
       "  '<sep>',\n",
       "  ' Market',\n",
       "  ' Efficiency',\n",
       "  '<sep>',\n",
       "  ' Strategic',\n",
       "  ' Pricing',\n",
       "  '<sep>',\n",
       "  ' Graph',\n",
       "  ' Theory',\n",
       "  '<sep>',\n",
       "  ' Matching',\n",
       "  ' Market',\n",
       "  '<sep>',\n",
       "  ' Social',\n",
       "  ' Opt',\n",
       "  'imal',\n",
       "  ' Allocation',\n",
       "  '<sep>',\n",
       "  ' Graph',\n",
       "  ' Structure',\n",
       "  '<sep>',\n",
       "  ' Competition',\n",
       "  ' among',\n",
       "  ' Tr',\n",
       "  'aders']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separated_list[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "581fafce-a20e-471f-b3fa-4bf590ec3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "[['grid',\n",
    "  'service',\n",
    "  'discovery',\n",
    "  '<sep>',\n",
    "  'uddi',\n",
    "  '<sep>',\n",
    "  'grid',\n",
    "  'computing',\n",
    "  '<sep>',\n",
    "  'web',\n",
    "  'service',\n",
    "  '<sep>',\n",
    "  'distributed',\n",
    "  'hash',\n",
    "  'tables',\n",
    "  '<sep>',\n",
    "  'distributed',\n",
    "  'hash',\n",
    "  'tables',\n",
    "  '<sep>',\n",
    "  'grid',\n",
    "  'computing'],\n",
    " ['grid',\n",
    "  'computing',\n",
    "  '<sep>',\n",
    "  'performance',\n",
    "  'modeling',\n",
    "  '<sep>',\n",
    "  'performance',\n",
    "  'evaluation',\n",
    "  '<sep>',\n",
    "  'adaptive',\n",
    "  'systems']]\n",
    "token_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "784d1b28-4199-44a5-8fc5-d8507b8b8597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Web', ' search', ' interaction'],\n",
       " [' popular', ' destinations'],\n",
       " [' search', ' results'],\n",
       " [' user', ' behavior'],\n",
       " [' query', '-level', ' granularity'],\n",
       " [' web', ' search'],\n",
       " [' search', ' interaction', ' feature'],\n",
       " [' authority', ' resources'],\n",
       " [' information', ' needs'],\n",
       " [' explor', 'atory', ' tasks'],\n",
       " [' query', ' topic']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6090f10d-f176-49b3-a2c6-69b85fc64218",
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyphrases in result:\n",
    "    lengths = count_words_in_keyphrases(keyphrases[0])\n",
    "    print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8400f1c-6011-4162-9650-22c04dd3dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_keyphrases(keyphrases):\n",
    "    # Remove semicolons\n",
    "    keyphrases = keyphrases.replace(';', '')\n",
    "    # Split keyphrases by commas and strip whitespace\n",
    "    phrases = [phrase.strip() for phrase in keyphrases.split(',') if phrase.strip()]\n",
    "    # Count the number of words in each phrase\n",
    "    lengths = [len(phrase.split()) for phrase in phrases]\n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95207281-6b71-4491-8f27-40a3cf33c636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 182, 2: 413, 3: 121, 4: 22, 5: 4}\n"
     ]
    }
   ],
   "source": [
    "length_counts = defaultdict(int)\n",
    "# Process each list in the result\n",
    "for keyphrases in result:\n",
    "    lengths = count_words_in_keyphrases(keyphrases[0])\n",
    "    for length in lengths:\n",
    "        length_counts[length] += 1\n",
    "\n",
    "# Convert defaultdict to a regular dict for cleaner output\n",
    "length_counts = dict(length_counts)\n",
    "\n",
    "# Print the dictionary of counts\n",
    "print(length_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad6cc5ac-fc10-4ce2-aec9-22dbb01715b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "182+413+121+26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc2de3-a933-4cd8-afe1-de09f86a63f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

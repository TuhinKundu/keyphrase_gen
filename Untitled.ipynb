{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "384b3d60-a867-479c-bd78-27b8dec91457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import *\n",
    "def load_llama(dataset):\n",
    "    with open('graph_outputs/'+dataset+'_llama3_kpp_test.json') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    kp_predictions = json_data['dup_removed_kp_predictions']\n",
    "    probabilities = json_data['probabilities']\n",
    "    token_predictions = json_data['dup_removed_token_predictions']\n",
    "    src = json_data['src']\n",
    "    targets = json_data['targets']\n",
    "    all_kpp_values = json_data['dup_removed_kpp']\n",
    "\n",
    "    return kp_predictions, probabilities, token_predictions, src, targets, all_kpp_values\n",
    "\n",
    "def json_load_one2seq(model, dataset):\n",
    "    with open('graph_outputs/'+model+dataset+'_all_output.json', 'r') as f:\n",
    "        dic= json.load(f)\n",
    "\n",
    "    scores = dic['scores']\n",
    "    predictions = dic['predictions']\n",
    "    entropies = dic['entropies']\n",
    "    context_lines = dic['context_lines']\n",
    "\n",
    "    return scores, predictions,entropies,context_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f330427-bd09-456b-bfcf-8be0c4422562",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_predictions(predictions, beam=False):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    processed_predictions = []\n",
    "    for beam_prediction in predictions:\n",
    "        if beam:\n",
    "            prediction_ = \"\"\n",
    "            for prediction in beam_prediction:\n",
    "                prediction = prediction.replace(\";\", \"<sep>\")\n",
    "                prediction = prediction.split(\"<eos>\")[0]\n",
    "                if not prediction_:\n",
    "                    prediction_ += prediction\n",
    "                else:\n",
    "                    prediction_ += ' <sep> ' + prediction\n",
    "            prediction = prediction_\n",
    "        else:\n",
    "            beam_prediction = beam_prediction.replace(\";\", \"<sep>\")\n",
    "            prediction = beam_prediction.split(\"<eos>\")[0]\n",
    "\n",
    "        prediction = prediction.split(\",\")\n",
    "\n",
    "        stemed_prediction = []\n",
    "        for kp in prediction:\n",
    "            kp = kp.lower().strip()\n",
    "            if kp != \"\" and kp != \"<peos>\" and kp!=\",\" and kp != \".\" and kp != \"<unk>\":  # and \".\" not in kp and \",\" not in kp\n",
    "                tokenized_kp = kp.split(\" \")  # nltk.word_tokenize(kp)\n",
    "                tokenized_stemed_kp = [stemmer.stem(kw).strip() for kw in tokenized_kp]\n",
    "                stemed_kp = \" \".join(tokenized_stemed_kp).replace(\"< digit >\", \"<digit>\")\n",
    "                if stemed_kp.strip() != \"\":\n",
    "                    stemed_prediction.append(stemed_kp.strip())\n",
    "\n",
    "        # make prediction duplicates free but preserve order for @topk\n",
    "\n",
    "        prediction_dict = {}\n",
    "        stemed_prediction_ = []\n",
    "        for kp in stemed_prediction:\n",
    "            if kp not in prediction_dict:\n",
    "                prediction_dict[kp] = 1\n",
    "                stemed_prediction_.append(kp)\n",
    "        stemed_prediction = stemed_prediction_\n",
    "\n",
    "        processed_predictions.extend(stemed_prediction)\n",
    "\n",
    "    return processed_predictions\n",
    "\n",
    "def process_srcs(srcs):\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_srcs = []\n",
    "    tokenized_src = srcs.split()  # Split the string into words\n",
    "    tokenized_stemed_src = [stemmer.stem(token.strip().lower()).strip() for token in tokenized_src]\n",
    "    stemed_src = \" \".join(tokenized_stemed_src).strip()\n",
    "    processed_srcs.append(stemed_src)\n",
    "    return processed_srcs\n",
    "\n",
    "def add_sep_and_kpp(token_predictions, all_kpp_values):\n",
    "    new_token_list = []\n",
    "    new_kpp_list = []\n",
    "\n",
    "    for tokens, kpp_values in zip(token_predictions, all_kpp_values):\n",
    "        merged_tokens = []\n",
    "        merged_kpp = []\n",
    "        kpp_index = 0\n",
    "\n",
    "        for i, sublist in enumerate(tokens):\n",
    "            merged_tokens.extend(sublist)\n",
    "            merged_kpp.extend(kpp_values[kpp_index:kpp_index + len(sublist)])\n",
    "            kpp_index += len(sublist)\n",
    "\n",
    "            if i < len(tokens) - 1:\n",
    "                merged_tokens.append('<sep>')\n",
    "                merged_kpp.append(0)  # Add corresponding 0 for <sep>\n",
    "\n",
    "        new_token_list.append(merged_tokens)\n",
    "        new_kpp_list.append(merged_kpp)\n",
    "\n",
    "    return new_token_list, new_kpp_list\n",
    "\n",
    "def get_one2seq_ppl(predictions, scores, context_lines):\n",
    "    present_ppl, absent_ppl = [], []\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        stemmed_context = stem_text(context_lines[i])\n",
    "        kp_collect = []\n",
    "        prob_collect = []\n",
    "        for j, token in enumerate(pred):\n",
    "\n",
    "            if token == \"<sep>\":\n",
    "\n",
    "                if len(kp_collect) > 0:\n",
    "                    ppl = np.prod(prob_collect) ** (-1 / float(len(prob_collect)))\n",
    "                    stemmed_kp = stem_text(' '.join(kp_collect))\n",
    "                    #print(prob_collect)\n",
    "                    #print(kp_collect)\n",
    "                    #print(ppl)\n",
    "                    if stemmed_kp in stemmed_context:\n",
    "                        present_ppl.append(ppl)\n",
    "                    else:\n",
    "                        #print(stemmed_kp)\n",
    "                        #print(stemmed_context)\n",
    "                        #print(pred)\n",
    "                        #print()\n",
    "                        absent_ppl.append(ppl)\n",
    "                    kp_collect, prob_collect = [], []\n",
    "            else:\n",
    "                if len(pred)!=len(scores[i]):\n",
    "                    print(kp_collect, len(pred))\n",
    "                    break\n",
    "                kp_collect.append(token)\n",
    "                prob_collect.append(scores[i][j])\n",
    "        if len(kp_collect) > 0:\n",
    "            ppl = np.prod(prob_collect) ** (-1 / float(len(prob_collect)))\n",
    "            stemmed_kp = stem_text(' '.join(kp_collect))\n",
    "            if stemmed_kp in stemmed_context:\n",
    "                present_ppl.append(ppl)\n",
    "            else:\n",
    "                absent_ppl.append(ppl)\n",
    "    return present_ppl, absent_ppl\n",
    "stemmer = PorterStemmer()\n",
    "def stem_text(text):\n",
    "\n",
    "    return ' '.join([stemmer.stem(w) for w in text.strip().split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "387fa155-be17-49af-a7ee-8616d702f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, predictions,entropies,context_lines= json_load_one2seq('one2seq_','semeval')\n",
    "\n",
    "present_ppl, absent_ppl = get_one2seq_ppl(predictions, scores, context_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab2067-0b5d-4bb4-913b-52ed6edb363b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "066f49d9-4830-45f5-b8eb-de81de744e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] 57\n",
      "[] 51\n",
      "[] 43\n"
     ]
    }
   ],
   "source": [
    "kp_predictions, probabilities, token_predictions, src, targets, all_kpp_values=load_llama('semeval')\n",
    "context_lines=src\n",
    "token_predictions,scores=add_sep_and_kpp(token_predictions, probabilities)\n",
    "predictions=[]\n",
    "\n",
    "for i in range(len(src)):\n",
    "    temp_predictions = process_predictions(token_predictions[i])\n",
    "    predictions.append(temp_predictions)\n",
    "\n",
    "present_ppl, absent_ppl = get_one2seq_ppl(predictions, scores, context_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8feac769-b0ae-4497-922a-2e8f5a35245f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(present_ppl).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "926b1ebf-e810-4576-b2cd-aad97856d000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['grid',\n",
       "  'service',\n",
       "  'discovery',\n",
       "  '<sep>',\n",
       "  'uddi',\n",
       "  '<sep>',\n",
       "  'grid',\n",
       "  'computing',\n",
       "  '<sep>',\n",
       "  'web',\n",
       "  'service',\n",
       "  '<sep>',\n",
       "  'distributed',\n",
       "  'hash',\n",
       "  'tables',\n",
       "  '<sep>',\n",
       "  'distributed',\n",
       "  'hash',\n",
       "  'tables',\n",
       "  '<sep>',\n",
       "  'grid',\n",
       "  'computing'],\n",
       " ['grid',\n",
       "  'computing',\n",
       "  '<sep>',\n",
       "  'performance',\n",
       "  'modeling',\n",
       "  '<sep>',\n",
       "  'performance',\n",
       "  'evaluation',\n",
       "  '<sep>',\n",
       "  'adaptive',\n",
       "  'systems']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94f976ca-7e28-4bcd-b01e-8ce10dd256db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama(dataset):\n",
    "    with open('graph_outputs/'+dataset+'_llama3_kpp_test.json') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    kp_predictions = json_data['dup_removed_kp_predictions']\n",
    "    probabilities = json_data['probabilities']\n",
    "    token_predictions = json_data['dup_removed_token_predictions']\n",
    "    src = json_data['src']\n",
    "    targets = json_data['targets']\n",
    "    all_kpp_values = json_data['dup_removed_kpp']\n",
    "\n",
    "    return kp_predictions, probabilities, token_predictions, src, targets, all_kpp_values\n",
    "\n",
    "\n",
    "kp_predictions, probabilities, token_predictions, src, targets, all_kpp_values=load_llama(\"semeval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5339ab2-3a07-4dbe-a24a-a8c1119a32a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Web', ' search', ' interaction'],\n",
       "  [' popular', ' destinations'],\n",
       "  [' search', ' results'],\n",
       "  [' user', ' behavior'],\n",
       "  [' query', '-level', ' granularity'],\n",
       "  [' web', ' search'],\n",
       "  [' search', ' interaction', ' feature'],\n",
       "  [' authority', ' resources'],\n",
       "  [' information', ' needs'],\n",
       "  [' explor', 'atory', ' tasks'],\n",
       "  [' query', ' topic']],\n",
       " [['Trading', ' Networks'],\n",
       "  [' Price', '-', 'Setting', ' Agents'],\n",
       "  [' Nash', ' Equ', 'ilibrium'],\n",
       "  [' Game', ' Theory'],\n",
       "  [' Market', ' Efficiency'],\n",
       "  [' Strategic', ' Pricing'],\n",
       "  [' Graph', ' Theory'],\n",
       "  [' Matching', ' Market'],\n",
       "  [' Social', ' Opt', 'imal', ' Allocation'],\n",
       "  [' Graph', ' Structure'],\n",
       "  [' Competition', ' among', ' Tr', 'aders']]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_predictions[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "800088ce-bbb6-4df7-b7a0-7ef5c9f750dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sep_and_kpp(token_predictions, all_kpp_values):\n",
    "    new_token_list = []\n",
    "    new_kpp_list = []\n",
    "\n",
    "    for tokens, kpp_values in zip(token_predictions, all_kpp_values):\n",
    "        merged_tokens = []\n",
    "        merged_kpp = []\n",
    "        kpp_index = 0\n",
    "\n",
    "        for i, sublist in enumerate(tokens):\n",
    "            merged_tokens.extend(sublist)\n",
    "            merged_kpp.extend(kpp_values[kpp_index:kpp_index + len(sublist)])\n",
    "            kpp_index += len(sublist)\n",
    "\n",
    "            if i < len(tokens) - 1:\n",
    "                merged_tokens.append('<sep>')\n",
    "                merged_kpp.append(0)  # Add corresponding 0 for <sep>\n",
    "\n",
    "        new_token_list.append(merged_tokens)\n",
    "        new_kpp_list.append(merged_kpp)\n",
    "\n",
    "    return new_token_list, new_kpp_list\n",
    "new_token_predictions, new_kpp_values = add_sep_and_kpp(token_predictions, probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e735476-275c-4c09-92a7-e40aae32dd9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Web',\n",
       " ' search',\n",
       " ' interaction',\n",
       " '<sep>',\n",
       " ' popular',\n",
       " ' destinations',\n",
       " '<sep>',\n",
       " ' search',\n",
       " ' results',\n",
       " '<sep>',\n",
       " ' user',\n",
       " ' behavior',\n",
       " '<sep>',\n",
       " ' query',\n",
       " '-level',\n",
       " ' granularity',\n",
       " '<sep>',\n",
       " ' web',\n",
       " ' search',\n",
       " '<sep>',\n",
       " ' search',\n",
       " ' interaction',\n",
       " ' feature',\n",
       " '<sep>',\n",
       " ' authority',\n",
       " ' resources',\n",
       " '<sep>',\n",
       " ' information',\n",
       " ' needs',\n",
       " '<sep>',\n",
       " ' explor',\n",
       " 'atory',\n",
       " ' tasks',\n",
       " '<sep>',\n",
       " ' query',\n",
       " ' topic']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_token_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "484833e7-c1df-47d8-a3b6-90bf3563f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predictions(predictions, beam=False):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    processed_predictions = []\n",
    "    for beam_prediction in predictions:\n",
    "        if beam:\n",
    "            prediction_ = \"\"\n",
    "            for prediction in beam_prediction:\n",
    "                prediction = prediction.replace(\";\", \"<sep>\")\n",
    "                prediction = prediction.split(\"<eos>\")[0]\n",
    "                if not prediction_:\n",
    "                    prediction_ += prediction\n",
    "                else:\n",
    "                    prediction_ += ' <sep> ' + prediction\n",
    "            prediction = prediction_\n",
    "        else:\n",
    "            beam_prediction = beam_prediction.replace(\";\", \"<sep>\")\n",
    "            prediction = beam_prediction.split(\"<eos>\")[0]\n",
    "\n",
    "        prediction = prediction.split(\",\")\n",
    "\n",
    "        stemed_prediction = []\n",
    "        for kp in prediction:\n",
    "            kp = kp.lower().strip()\n",
    "            if kp != \"\" and kp != \"<peos>\" and kp!=\",\" and kp != \".\" and kp != \"<unk>\":  # and \".\" not in kp and \",\" not in kp\n",
    "                tokenized_kp = kp.split(\" \")  # nltk.word_tokenize(kp)\n",
    "                tokenized_stemed_kp = [stemmer.stem(kw).strip() for kw in tokenized_kp]\n",
    "                stemed_kp = \" \".join(tokenized_stemed_kp).replace(\"< digit >\", \"<digit>\")\n",
    "                if stemed_kp.strip() != \"\":\n",
    "                    stemed_prediction.append(stemed_kp.strip())\n",
    "\n",
    "        # make prediction duplicates free but preserve order for @topk\n",
    "\n",
    "        prediction_dict = {}\n",
    "        stemed_prediction_ = []\n",
    "        for kp in stemed_prediction:\n",
    "            if kp not in prediction_dict:\n",
    "                prediction_dict[kp] = 1\n",
    "                stemed_prediction_.append(kp)\n",
    "        stemed_prediction = stemed_prediction_\n",
    "\n",
    "        processed_predictions.extend(stemed_prediction)\n",
    "\n",
    "    return processed_predictions\n",
    "\n",
    "def process_srcs(srcs):\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_srcs = []\n",
    "    tokenized_src = srcs.split()  # Split the string into words\n",
    "    tokenized_stemed_src = [stemmer.stem(token.strip().lower()).strip() for token in tokenized_src]\n",
    "    stemed_src = \" \".join(tokenized_stemed_src).strip()\n",
    "    processed_srcs.append(stemed_src)\n",
    "    return processed_srcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3f93f6b0-a0fc-48d6-bda1-d7bead016115",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=['Web',\n",
    " ' search',\n",
    " ' interaction',\n",
    " '<sep>',\n",
    " ' popular',\n",
    " ' destinations',\n",
    " '<sep>',\n",
    " ' search',\n",
    " ' results',\n",
    " '<sep>',\n",
    " ' user',\n",
    " ' behavior',\n",
    " '<sep>',\n",
    " ' query',\n",
    " '-level',\n",
    " ' granularity',\n",
    " '<sep>',\n",
    " ' web',\n",
    " ' search',\n",
    " '<sep>',\n",
    " ' search',\n",
    " ' interaction',\n",
    " ' feature',\n",
    " '<sep>',\n",
    " ' authority',\n",
    " ' resources',\n",
    " '<sep>',\n",
    " ' information',\n",
    " ' needs',\n",
    " '<sep>',\n",
    " ' explor',\n",
    " 'atory',\n",
    " ' tasks',\n",
    " '<sep>',\n",
    " ' query',\n",
    " ' topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d54659c8-6aa2-451e-8139-05c0a01e472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predictions(predictions, beam=False):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    processed_predictions = []\n",
    "    for beam_prediction in predictions:\n",
    "        if beam:\n",
    "            prediction_ = \"\"\n",
    "            for prediction in beam_prediction:\n",
    "                prediction = prediction.replace(\";\", \"<sep>\")\n",
    "                prediction = prediction.split(\"<eos>\")[0]\n",
    "                if not prediction_:\n",
    "                    prediction_ += prediction\n",
    "                else:\n",
    "                    prediction_ += ' <sep> ' + prediction\n",
    "            prediction = prediction_\n",
    "        else:\n",
    "            beam_prediction = beam_prediction.replace(\";\", \"<sep>\")\n",
    "            prediction = beam_prediction.split(\"<eos>\")[0]\n",
    "\n",
    "        prediction = prediction.split(\",\")\n",
    "\n",
    "        stemed_prediction = []\n",
    "        for kp in prediction:\n",
    "            kp = kp.lower().strip()\n",
    "            if kp != \"\" and kp != \"<peos>\" and kp!=\",\" and kp != \".\" and kp != \"<unk>\":  # and \".\" not in kp and \",\" not in kp\n",
    "                tokenized_kp = kp.split(\" \")  # nltk.word_tokenize(kp)\n",
    "                tokenized_stemed_kp = [stemmer.stem(kw).strip() for kw in tokenized_kp]\n",
    "                stemed_kp = \" \".join(tokenized_stemed_kp).replace(\"< digit >\", \"<digit>\")\n",
    "                if stemed_kp.strip() != \"\":\n",
    "                    stemed_prediction.append(stemed_kp.strip())\n",
    "\n",
    "        # make prediction duplicates free but preserve order for @topk\n",
    "\n",
    "        prediction_dict = {}\n",
    "        stemed_prediction_ = []\n",
    "        for kp in stemed_prediction:\n",
    "            if kp not in prediction_dict:\n",
    "                prediction_dict[kp] = 1\n",
    "                stemed_prediction_.append(kp)\n",
    "        stemed_prediction = stemed_prediction_\n",
    "\n",
    "        processed_predictions.extend(stemed_prediction)\n",
    "\n",
    "    return processed_predictions\n",
    "\n",
    "def process_srcs(srcs):\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_srcs = []\n",
    "    tokenized_src = srcs.split()  # Split the string into words\n",
    "    tokenized_stemed_src = [stemmer.stem(token.strip().lower()).strip() for token in tokenized_src]\n",
    "    stemed_src = \" \".join(tokenized_stemed_src).strip()\n",
    "    processed_srcs.append(stemed_src)\n",
    "    return processed_srcs\n",
    "\n",
    "def probab_llama_boxplots(dataset): #taken from line 811 one2seq code and adapted for llama\n",
    "\n",
    "    kp_predictions, probabilities, token_predictions, src, targets, all_kpp_values=load_llama(dataset)\n",
    "    context_lines=src\n",
    "    token_predictions,scores=add_sep_and_kpp(token_predictions, probabilities)\n",
    "    \n",
    "    predictions=[]\n",
    "    srcs=[]\n",
    "    \n",
    "    for i in range(len(src)):\n",
    "        temp_predictions = process_predictions(token_predictions[i])\n",
    "        #temp_srcs = process_srcs(src[i])\n",
    "        predictions.append(temp_predictions)\n",
    "        #srcs.append(temp_srcs)\n",
    "    print(predictions[0:2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5645ca97-57c7-4a03-aef6-69a8cd589732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['web', 'search', 'interact', '<sep>', 'popular', 'destin', '<sep>', 'search', 'result', '<sep>', 'user', 'behavior', '<sep>', 'queri', '-level', 'granular', '<sep>', 'web', 'search', '<sep>', 'search', 'interact', 'featur', '<sep>', 'author', 'resourc', '<sep>', 'inform', 'need', '<sep>', 'explor', 'atori', 'task', '<sep>', 'queri', 'topic'], ['trade', 'network', '<sep>', 'price', '-', 'set', 'agent', '<sep>', 'nash', 'equ', 'ilibrium', '<sep>', 'game', 'theori', '<sep>', 'market', 'effici', '<sep>', 'strateg', 'price', '<sep>', 'graph', 'theori', '<sep>', 'match', 'market', '<sep>', 'social', 'opt', 'imal', 'alloc', '<sep>', 'graph', 'structur', '<sep>', 'competit', 'among', 'tr', 'ader']]\n"
     ]
    }
   ],
   "source": [
    "probab_llama_boxplots(\"semeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa798a5e-73bb-4dd8-97e1-f4a9e67878ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "[['grid',\n",
    "  'service',\n",
    "  'discovery',\n",
    "  '<sep>',\n",
    "  'uddi',\n",
    "  '<sep>',\n",
    "  'grid',\n",
    "  'computing',\n",
    "  '<sep>',\n",
    "  'web',\n",
    "  'service',\n",
    "  '<sep>',\n",
    "  'distributed',\n",
    "  'hash',\n",
    "  'tables',\n",
    "  '<sep>',\n",
    "  'distributed',\n",
    "  'hash',\n",
    "  'tables',\n",
    "  '<sep>',\n",
    "  'grid',\n",
    "  'computing'],\n",
    " ['grid',\n",
    "  'computing',\n",
    "  '<sep>',\n",
    "  'performance',\n",
    "  'modeling',\n",
    "  '<sep>',\n",
    "  'performance',\n",
    "  'evaluation',\n",
    "  '<sep>',\n",
    "  'adaptive',\n",
    "  'systems']]\n",
    "token_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66bb7c66-b4c5-4c6b-8427-3c611ee15515",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "srcs=[]\n",
    "\n",
    "for i in range(len(src)):\n",
    "    temp_predictions = process_predictions(token_predictions[i][0])\n",
    "    temp_srcs = process_srcs(src[i])\n",
    "    predictions.append(temp_predictions)\n",
    "    srcs.append(temp_srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e20b87b-64ab-421a-b00c-41f2122d8c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Web', ' search', ' interaction']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee3ff8-c514-46e2-8160-c8e1d9f9e9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91f080e8-4ba8-4970-ba40-f511abc594f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "separated_list=combine_with_separator(token_predictions)\n",
    "sep_kpp=combine_with_separator1(all_kpp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06b1ea8e-f7f0-4ecc-9afa-89826cbd1c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.1497876701364411,\n",
       "  1.1143242899866512,\n",
       "  2.3891409365149947,\n",
       "  1.7717787304326484,\n",
       "  1.2276765139403374,\n",
       "  5.770539652041531,\n",
       "  1.9125475609114326,\n",
       "  3.736857856164639,\n",
       "  3.7247930349605762,\n",
       "  1.0913318776857792,\n",
       "  3.0313200339968684,\n",
       "  '<sep>',\n",
       "  0],\n",
       " [1.5681105111721405,\n",
       "  1.0073730011965898,\n",
       "  4.972799635990106,\n",
       "  2.3086282395514695,\n",
       "  2.849925088934097,\n",
       "  6.806432206131389,\n",
       "  1.832622051593755,\n",
       "  1.7361748951641185,\n",
       "  3.780131091318533,\n",
       "  9.331080740925575,\n",
       "  1.1584410684403077,\n",
       "  '<sep>',\n",
       "  0]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep_kpp[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5171a0ac-129e-496a-99aa-5e6a35a25a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Web',\n",
       "  ' search',\n",
       "  ' interaction',\n",
       "  '<sep>',\n",
       "  ' popular',\n",
       "  ' destinations',\n",
       "  '<sep>',\n",
       "  ' search',\n",
       "  ' results',\n",
       "  '<sep>',\n",
       "  ' user',\n",
       "  ' behavior',\n",
       "  '<sep>',\n",
       "  ' query',\n",
       "  '-level',\n",
       "  ' granularity',\n",
       "  '<sep>',\n",
       "  ' web',\n",
       "  ' search',\n",
       "  '<sep>',\n",
       "  ' search',\n",
       "  ' interaction',\n",
       "  ' feature',\n",
       "  '<sep>',\n",
       "  ' authority',\n",
       "  ' resources',\n",
       "  '<sep>',\n",
       "  ' information',\n",
       "  ' needs',\n",
       "  '<sep>',\n",
       "  ' explor',\n",
       "  'atory',\n",
       "  ' tasks',\n",
       "  '<sep>',\n",
       "  ' query',\n",
       "  ' topic'],\n",
       " ['Trading',\n",
       "  ' Networks',\n",
       "  '<sep>',\n",
       "  ' Price',\n",
       "  '-',\n",
       "  'Setting',\n",
       "  ' Agents',\n",
       "  '<sep>',\n",
       "  ' Nash',\n",
       "  ' Equ',\n",
       "  'ilibrium',\n",
       "  '<sep>',\n",
       "  ' Game',\n",
       "  ' Theory',\n",
       "  '<sep>',\n",
       "  ' Market',\n",
       "  ' Efficiency',\n",
       "  '<sep>',\n",
       "  ' Strategic',\n",
       "  ' Pricing',\n",
       "  '<sep>',\n",
       "  ' Graph',\n",
       "  ' Theory',\n",
       "  '<sep>',\n",
       "  ' Matching',\n",
       "  ' Market',\n",
       "  '<sep>',\n",
       "  ' Social',\n",
       "  ' Opt',\n",
       "  'imal',\n",
       "  ' Allocation',\n",
       "  '<sep>',\n",
       "  ' Graph',\n",
       "  ' Structure',\n",
       "  '<sep>',\n",
       "  ' Competition',\n",
       "  ' among',\n",
       "  ' Tr',\n",
       "  'aders']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separated_list[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "581fafce-a20e-471f-b3fa-4bf590ec3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "[['grid',\n",
    "  'service',\n",
    "  'discovery',\n",
    "  '<sep>',\n",
    "  'uddi',\n",
    "  '<sep>',\n",
    "  'grid',\n",
    "  'computing',\n",
    "  '<sep>',\n",
    "  'web',\n",
    "  'service',\n",
    "  '<sep>',\n",
    "  'distributed',\n",
    "  'hash',\n",
    "  'tables',\n",
    "  '<sep>',\n",
    "  'distributed',\n",
    "  'hash',\n",
    "  'tables',\n",
    "  '<sep>',\n",
    "  'grid',\n",
    "  'computing'],\n",
    " ['grid',\n",
    "  'computing',\n",
    "  '<sep>',\n",
    "  'performance',\n",
    "  'modeling',\n",
    "  '<sep>',\n",
    "  'performance',\n",
    "  'evaluation',\n",
    "  '<sep>',\n",
    "  'adaptive',\n",
    "  'systems']]\n",
    "token_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "784d1b28-4199-44a5-8fc5-d8507b8b8597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Web', ' search', ' interaction'],\n",
       " [' popular', ' destinations'],\n",
       " [' search', ' results'],\n",
       " [' user', ' behavior'],\n",
       " [' query', '-level', ' granularity'],\n",
       " [' web', ' search'],\n",
       " [' search', ' interaction', ' feature'],\n",
       " [' authority', ' resources'],\n",
       " [' information', ' needs'],\n",
       " [' explor', 'atory', ' tasks'],\n",
       " [' query', ' topic']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6090f10d-f176-49b3-a2c6-69b85fc64218",
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyphrases in result:\n",
    "    lengths = count_words_in_keyphrases(keyphrases[0])\n",
    "    print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8400f1c-6011-4162-9650-22c04dd3dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_keyphrases(keyphrases):\n",
    "    # Remove semicolons\n",
    "    keyphrases = keyphrases.replace(';', '')\n",
    "    # Split keyphrases by commas and strip whitespace\n",
    "    phrases = [phrase.strip() for phrase in keyphrases.split(',') if phrase.strip()]\n",
    "    # Count the number of words in each phrase\n",
    "    lengths = [len(phrase.split()) for phrase in phrases]\n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95207281-6b71-4491-8f27-40a3cf33c636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 182, 2: 413, 3: 121, 4: 22, 5: 4}\n"
     ]
    }
   ],
   "source": [
    "length_counts = defaultdict(int)\n",
    "# Process each list in the result\n",
    "for keyphrases in result:\n",
    "    lengths = count_words_in_keyphrases(keyphrases[0])\n",
    "    for length in lengths:\n",
    "        length_counts[length] += 1\n",
    "\n",
    "# Convert defaultdict to a regular dict for cleaner output\n",
    "length_counts = dict(length_counts)\n",
    "\n",
    "# Print the dictionary of counts\n",
    "print(length_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad6cc5ac-fc10-4ce2-aec9-22dbb01715b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "182+413+121+26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc2de3-a933-4cd8-afe1-de09f86a63f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
